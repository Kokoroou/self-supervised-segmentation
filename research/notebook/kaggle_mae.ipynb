{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Connection\n","*Kaggle do not support user input, that's why I cannot use Rclone to connect to my OneDrive storage for code and data*\n","\n","*-> My solution: Save data on Kaggle input directory, code sync by Github (git clone) and save checkpoint on Kaggle working directory*"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:30.031431Z","iopub.status.busy":"2023-06-28T20:50:30.030859Z","iopub.status.idle":"2023-06-28T20:50:30.062224Z","shell.execute_reply":"2023-06-28T20:50:30.061027Z","shell.execute_reply.started":"2023-06-28T20:50:30.031392Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","\n","os.chdir(\"/kaggle/working\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:30.076232Z","iopub.status.busy":"2023-06-28T20:50:30.073439Z","iopub.status.idle":"2023-06-28T20:50:32.233372Z","shell.execute_reply":"2023-06-28T20:50:32.231771Z","shell.execute_reply.started":"2023-06-28T20:50:30.076187Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n","__notebook_source__.ipynb\n"]}],"source":["!cd ./ && pwd && ls"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:32.240991Z","iopub.status.busy":"2023-06-28T20:50:32.239401Z","iopub.status.idle":"2023-06-28T20:50:32.248611Z","shell.execute_reply":"2023-06-28T20:50:32.246978Z","shell.execute_reply.started":"2023-06-28T20:50:32.240940Z"},"trusted":true},"outputs":[],"source":["# from kaggle_secrets import UserSecretsClient\n","\n","\n","# user_secrets = UserSecretsClient()\n","# github_token = user_secrets.get_secret(\"github_token\")\n","\n","# os.system(f\"git clone -b develop https://{github_token}@github.com/Kokoroou/self-supervised-segmentation\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Preparation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Main config to prepare data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Config\n","data_source = \"kokoroou/polypgen2021\"\n","data_dir = \"./input\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Preprocessing data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install --upgrade pip\n","%pip install kaggle\n","%pip install torchvision"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pathlib import Path, PurePosixPath\n","import time\n","\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","\n","\n","# Download data from Kaggle (Only need on Colab)\n","# !kaggle datasets download -d {data_source} -p {data_dir}\n","\n","# Define the path to the directory containing the train images\n","current_dir = Path(os.getcwd())\n","source_dir = Path(data_dir) / \"polypgen2021\" / \"PolypGen2021_MultiCenterData_v3\"\n","\n","# Define the path to the file containing the train image names\n","train_filepath = source_dir / \"train_autoencoder.txt\"\n","test_filepath = source_dir / \"test_autoencoder.txt\"\n","\n","# Open the file with names of training, testing image file, then make DataLoader\n","with open(train_filepath, \"r\") as f:\n","    train_filenames = f.read().splitlines()\n","with open(test_filepath, \"r\") as f:\n","    test_filenames = f.read().splitlines()\n","\n","# Create a custom dataset class to load the images\n","class CustomDataset(ImageFolder):\n","    def __init__(self, root, names, transform=None):\n","        super().__init__(root, transform=transform)\n","        self.samples = [\n","            (Path(root, PurePosixPath(name)), 0) for name in names\n","        ]\n","\n","# Define the transformations to apply to the images\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","start = time.time()\n","print(\"Create dataset...\")\n","# Create an instance of the custom dataset\n","train_dataset = CustomDataset(str(source_dir), train_filenames, transform=transform)\n","test_dataset = CustomDataset(str(source_dir), test_filenames, transform=transform)\n","\n","# Create a data loader to load the images in batches\n","batch_size = 64\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","print(f\"Create dataset. Finish in {time.time() - start}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Code"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Main config to prepare code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Config\n","repo_url = \"https://github.com/Kokoroou/self-supervised-segmentation\"\n","repo_dir = \"./src\"\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Prepare code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download repo from Github\n","!git clone -b develop {repo_url} {repo_dir}\n","\n","# Install requirements\n","%pip install -r \"./src/self-supervised-segmentation/requirements.txt\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Output"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Main config to save result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Config\n","checkpoint_dir = \"./checkpoint\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Connect to output storage\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Default storage in Kaggle\n"]},{"cell_type":"markdown","metadata":{},"source":["# Running"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use code on Github to train\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:32.253348Z","iopub.status.busy":"2023-06-28T20:50:32.252288Z","iopub.status.idle":"2023-06-28T20:50:35.904002Z","shell.execute_reply":"2023-06-28T20:50:35.903016Z","shell.execute_reply.started":"2023-06-28T20:50:32.253311Z"},"trusted":true},"outputs":[],"source":["from math import sqrt\n","\n","import torch\n","\n","\n","def unpatchify(patches, patch_size=16):\n","    \"\"\"\n","    Combining patches into images.\n","\n","    Args:\n","        patches: Input tensor with size (\n","            batch,\n","            (height / patch_size) * (width / patch_size),\n","            channels * patch_size * patch_size\n","            )\n","        patch_size: Patch size\n","    Returns:\n","        A batch of images with size (batch, channels, height, width)\n","    \"\"\"\n","    batch_size, num_patches, total_patch_size = patches.shape\n","    channels = total_patch_size // patch_size ** 2\n","\n","    # Count number of patches in height and width\n","    height_count = width_count = int(sqrt(num_patches))\n","\n","    # Calculate height and width of the image\n","    height = width = height_count * patch_size\n","\n","    # Raise error if num_patches is not a square number\n","    assert height_count * width_count == num_patches\n","\n","    # Unpatching patches into images\n","    patches = patches.reshape((batch_size, height_count, width_count, patch_size, patch_size, channels))\n","    patches = torch.einsum('nhwpqc->nchpwq', patches)\n","    images = patches.reshape((batch_size, channels, height, width))\n","\n","    return images\n","\n","\n","def random_masking(x, mask_ratio):\n","    \"\"\"\n","    Perform per-sample random masking by per-sample shuffling.\n","    Per-sample shuffling is done by argsort random noise.\n","\n","    Args:\n","        x: torch.Tensor, shape [N, L, D]\n","           The input sequence with embedded vector of elements of samples in batch.\n","           N: batch size, L: embedded vector length of a sample (number of elements), D: encoder embedding dimension\n","\n","        mask_ratio: float\n","           The ratio of elements to be masked (removed) from each sample.\n","\n","    Returns:\n","        kept_patches: torch.Tensor, shape [N, L_masked, D]\n","           The masked sequence with a subset of elements removed for each sample.\n","           L_masked: masked sequence length after removing elements.\n","\n","        mask: torch.Tensor, shape [N, L]\n","           Binary mask indicating which elements are kept (0) or removed (1) from each sequence.\n","\n","        ids_restore: torch.Tensor, shape [N, L]\n","           Indices used to restore the original order of the elements after shuffling.\n","\n","    \"\"\"\n","    batch_size, num_patches, dimension = x.shape\n","    keep_count = int(num_patches * (1 - mask_ratio))\n","\n","    noise = torch.rand(batch_size, num_patches, device=x.device)  # noise in [0, 1]\n","\n","    # Sort noise for each sample\n","    ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n","    ids_restore = torch.argsort(ids_shuffle, dim=1)\n","\n","    # Keep the first subset\n","    ids_keep = ids_shuffle[:, :keep_count]\n","    kept_patches = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, dimension))\n","\n","    # Generate the binary mask: 0 is keep, 1 is remove\n","    mask = torch.ones([batch_size, num_patches], device=x.device)\n","    mask[:, :keep_count] = 0\n","    # Un-shuffle to get the binary mask\n","    mask = torch.gather(mask, dim=1, index=ids_restore)\n","\n","    return kept_patches, mask, ids_restore\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:35.907046Z","iopub.status.busy":"2023-06-28T20:50:35.906506Z","iopub.status.idle":"2023-06-28T20:50:35.929145Z","shell.execute_reply":"2023-06-28T20:50:35.927740Z","shell.execute_reply.started":"2023-06-28T20:50:35.907019Z"},"trusted":true},"outputs":[],"source":["# Copyright (c) Meta Platforms, Inc. and affiliates.\n","# All rights reserved.\n","\n","# This source code is licensed under the license found in the\n","# LICENSE file in the root directory of this source tree.\n","# --------------------------------------------------------\n","# Position embedding utils\n","# --------------------------------------------------------\n","\n","import numpy as np\n","\n","import torch\n","\n","# --------------------------------------------------------\n","# 2D sine-cosine position embedding\n","# References:\n","# Transformer: https://github.com/tensorflow/models/blob/master/official/nlp/transformer/model_utils.py\n","# MoCo v3: https://github.com/facebookresearch/moco-v3\n","# --------------------------------------------------------\n","def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n","    \"\"\"\n","    grid_size: int of the grid height and width\n","    return:\n","    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n","    \"\"\"\n","    grid_h = np.arange(grid_size, dtype=np.float32)\n","    grid_w = np.arange(grid_size, dtype=np.float32)\n","    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n","    grid = np.stack(grid, axis=0)\n","\n","    grid = grid.reshape([2, 1, grid_size, grid_size])\n","    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n","    if cls_token:\n","        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n","    return pos_embed\n","\n","\n","def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n","    assert embed_dim % 2 == 0\n","\n","    # use half of dimensions to encode grid_h\n","    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n","    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n","\n","    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n","    return emb\n","\n","\n","def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n","    \"\"\"\n","    embed_dim: output dimension for each position\n","    pos: a list of positions to be encoded: size (M,)\n","    out: (M, D)\n","    \"\"\"\n","    assert embed_dim % 2 == 0\n","    omega = np.arange(embed_dim // 2, dtype=float)\n","    omega /= embed_dim / 2.\n","    omega = 1. / 10000**omega  # (D/2,)\n","\n","    pos = pos.reshape(-1)  # (M,)\n","    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n","\n","    emb_sin = np.sin(out) # (M, D/2)\n","    emb_cos = np.cos(out) # (M, D/2)\n","\n","    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n","    return emb\n","\n","\n","# --------------------------------------------------------\n","# Interpolate position embeddings for high-resolution\n","# References:\n","# DeiT: https://github.com/facebookresearch/deit\n","# --------------------------------------------------------\n","def interpolate_pos_embed(model, checkpoint_model):\n","    if 'pos_embed' in checkpoint_model:\n","        pos_embed_checkpoint = checkpoint_model['pos_embed']\n","        embedding_size = pos_embed_checkpoint.shape[-1]\n","        num_patches = model.patch_embed.num_patches\n","        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n","        # height (== width) for the checkpoint position embedding\n","        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n","        # height (== width) for the new position embedding\n","        new_size = int(num_patches ** 0.5)\n","        # class_token and dist_token are kept unchanged\n","        if orig_size != new_size:\n","            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n","            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n","            # only the position tokens are interpolated\n","            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n","            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n","            pos_tokens = torch.nn.functional.interpolate(\n","                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n","            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n","            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n","            checkpoint_model['pos_embed'] = new_pos_embed\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:35.932752Z","iopub.status.busy":"2023-06-28T20:50:35.931705Z","iopub.status.idle":"2023-06-28T20:50:37.469565Z","shell.execute_reply":"2023-06-28T20:50:37.468502Z","shell.execute_reply.started":"2023-06-28T20:50:35.932691Z"},"trusted":true},"outputs":[],"source":["from functools import partial\n","from typing import Any\n","\n","import torch\n","import torch.nn as nn\n","from timm.models.vision_transformer import PatchEmbed, Block\n","from torchinfo import summary\n","\n","class MaskedAutoencoderViT(nn.Module):\n","    \"\"\"\n","    Masked Autoencoder with VisionTransformer backbone\n","    \"\"\"\n","    def __init__(self,\n","                 img_size: int = 224,\n","                 patch_size: int = 16,\n","                 in_chans: int = 3,\n","                 encoder_embed_dim: int = 1024, encoder_depth: int = 24, encoder_num_heads: int = 16,\n","                 decoder_embed_dim: int = 512, decoder_depth: int = 8, decoder_num_heads: int = 16,\n","                 mlp_ratio: float = 4.,\n","                 norm_layer: Any = nn.LayerNorm,\n","                 norm_pix_loss: bool = False):\n","        \"\"\"\n","        Initialize model structure\n","\n","        Args:\n","            img_size: Image size of input image (e.g. 224 for 224x224 image)\n","            patch_size: Size of each patch\n","            in_chans: Number of input channels (e.g. 3 for RGB)\n","            encoder_embed_dim: Embedding dimension of encoder\n","            encoder_depth: Number of encoder blocks\n","            encoder_num_heads: Number of heads in encoder\n","            decoder_embed_dim: Embedding dimension of decoder\n","            decoder_depth: Number of decoder blocks\n","            decoder_num_heads: Number of heads in decoder\n","            mlp_ratio: Ratio of MLP hidden dim to embedding dim\n","            norm_layer: Normalization layer\n","            norm_pix_loss: Whether to normalize pixel loss\n","        \"\"\"\n","        super().__init__()\n","\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.in_chans = in_chans\n","        self.encoder_embed_dim = encoder_embed_dim\n","        self.encoder_depth = encoder_depth\n","        self.encoder_num_heads = encoder_num_heads\n","        self.decoder_embed_dim = decoder_embed_dim\n","        self.decoder_depth = decoder_depth\n","        self.decoder_num_heads = decoder_num_heads\n","        self.mlp_ratio = mlp_ratio\n","        self.norm_layer = norm_layer\n","        self.norm_pix_loss = norm_pix_loss\n","\n","        # --------------------------------------------------------------------------\n","        # MAE encoder specifics\n","        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, encoder_embed_dim)\n","        num_patches = self.patch_embed.num_patches\n","\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, encoder_embed_dim))\n","        # fixed sin-cos embedding\n","        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, encoder_embed_dim), requires_grad=False)\n","\n","        self.blocks = nn.ModuleList([\n","            Block(encoder_embed_dim, encoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n","            for _ in range(encoder_depth)])\n","        self.norm = norm_layer(encoder_embed_dim)\n","        # --------------------------------------------------------------------------\n","\n","        # --------------------------------------------------------------------------\n","        # MAE decoder specifics\n","        self.decoder_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=True)\n","\n","        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n","\n","        # fixed sin-cos embedding\n","        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)\n","\n","        self.decoder_blocks = nn.ModuleList([\n","            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n","            for _ in range(decoder_depth)])\n","\n","        self.decoder_norm = norm_layer(decoder_embed_dim)\n","        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size ** 2 * in_chans, bias=True)  # decoder to patch\n","        # --------------------------------------------------------------------------\n","\n","        self.initialize_weights()\n","\n","    def initialize_weights(self):\n","        # initialization\n","        # initialize (and freeze) pos_embed by sin-cos embedding\n","        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.patch_embed.num_patches**.5),\n","                                            cls_token=True)\n","        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n","\n","        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1],\n","                                                    int(self.patch_embed.num_patches**.5), cls_token=True)\n","        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n","\n","        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n","        w = self.patch_embed.proj.weight.data\n","        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n","\n","        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n","        torch.nn.init.normal_(self.cls_token, std=.02)\n","        torch.nn.init.normal_(self.mask_token, std=.02)\n","\n","        # initialize nn.Linear and nn.LayerNorm\n","        self.apply(self._init_weights)\n","\n","    @staticmethod\n","    def _init_weights(m):\n","        if isinstance(m, nn.Linear):\n","            # we use xavier_uniform following official JAX ViT:\n","            torch.nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def forward_encoder(self, x, mask_ratio):\n","        \"\"\"\n","        Forward pass through the encoder of the neural network model.\n","\n","        Args:\n","            x (torch.Tensor): Input image tensor of shape (N, C, H, W).\n","            mask_ratio (float): Ratio of elements to mask during random masking.\n","\n","        Returns:\n","            torch.Tensor: Encoded output tensor.\n","            torch.Tensor: Mask tensor indicating the masked elements.\n","            torch.Tensor: Restored indices tensor for masked elements.\n","        \"\"\"\n","        # Divide image into patches and embed them\n","        x = self.patch_embed(x)\n","\n","        # Add positional embedding without classification token\n","        x = x + self.pos_embed[:, 1:, :]\n","\n","        # Masking image patches, only keep patches that unmasked and info for restoring\n","        x, mask, ids_restore = random_masking(x, mask_ratio)\n","\n","        # Append cls token\n","        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n","        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","\n","        # Apply Transformer blocks\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","\n","        return x, mask, ids_restore\n","\n","    def forward_decoder(self, x, ids_restore):\n","        # embed tokens\n","        x = self.decoder_embed(x)\n","\n","        # append mask tokens to sequence\n","        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n","        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n","        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n","        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n","\n","        # add pos embed\n","        x = x + self.decoder_pos_embed\n","\n","        # apply Transformer blocks\n","        for blk in self.decoder_blocks:\n","            x = blk(x)\n","        x = self.decoder_norm(x)\n","\n","        # predictor projection\n","        x = self.decoder_pred(x)\n","\n","        # remove cls token\n","        x = x[:, 1:, :]\n","\n","        x = unpatchify(x, self.patch_size)\n","\n","        return x\n","\n","    # def forward_loss(self, imgs, pred, mask):\n","    #     \"\"\"\n","    #     imgs: [N, 3, H, W]\n","    #     pred: [N, L, p*p*3]\n","    #     mask: [N, L], 0 is keep, 1 is remove,\n","    #     \"\"\"\n","    #     target = patchify(imgs, self.patch_size)\n","    #     if self.norm_pix_loss:\n","    #         mean = target.mean(dim=-1, keepdim=True)\n","    #         var = target.var(dim=-1, keepdim=True)\n","    #         target = (target - mean) / (var + 1.e-6)**.5\n","    #\n","    #     loss = (pred - target) ** 2\n","    #     loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n","    #\n","    #     loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n","    #     return loss\n","\n","    def forward(self, imgs, mask_ratio=0.75):\n","        \"\"\"\n","        Args:\n","            imgs: Batch of images (shape: [N, C, H, W])\n","            mask_ratio: Ratio of masked patches\n","\n","        Returns:\n","            loss: Masked autoencoder loss\n","            pred: Predicted patches\n","            mask: Mask of removed patches\n","        \"\"\"\n","        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n","        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n","        # loss = self.forward_loss(imgs, pred, mask)\n","        return pred"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:37.471378Z","iopub.status.busy":"2023-06-28T20:50:37.470983Z","iopub.status.idle":"2023-06-28T20:50:37.477166Z","shell.execute_reply":"2023-06-28T20:50:37.476101Z","shell.execute_reply.started":"2023-06-28T20:50:37.471342Z"},"trusted":true},"outputs":[],"source":["# import gc\n","# from tqdm import tqdm\n","\n","# for i in tqdm(range(10)):\n","#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","#     image_size = 224\n","#     model = MaskedAutoencoderViT(img_size=image_size).to(device)\n","\n","#     with torch.no_grad():\n","#         result = summary(model, (64, 3, image_size, image_size))\n","#     #     print(result)\n","#     #     print(\"\\nNumber of parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n","\n","#         # Can only free allocated GPU memory at the moment, but cannot free RAM\n","#         del result\n","#         gc.collect()\n","#         torch.cuda.empty_cache()\n","#         print(torch.cuda.memory_summary())\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:37.480084Z","iopub.status.busy":"2023-06-28T20:50:37.479208Z","iopub.status.idle":"2023-06-28T20:50:37.490096Z","shell.execute_reply":"2023-06-28T20:50:37.488926Z","shell.execute_reply.started":"2023-06-28T20:50:37.479941Z"},"trusted":true},"outputs":[],"source":["import argparse\n","\n","\n","parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n","\n","parser.add_argument(\"-b\", \"--batch_size\", type=int, default=64, help=\"Batch size\")\n","parser.add_argument(\"-e\", \"--epochs\", type=int, default=100, help=\"Number of training epochs\")\n","parser.add_argument(\"-lr\", \"--learning_rate\", type=int, default=0.001, help=\"Learning rate\")  \n","parser.add_argument(\"-o\", \"--output_dir\", type=str, default=\"./output_dir\", help=\"Directory for save checkpoint\")\n","\n","args = parser.parse_known_args()[0]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:37.495008Z","iopub.status.busy":"2023-06-28T20:50:37.494567Z","iopub.status.idle":"2023-06-28T20:50:48.467752Z","shell.execute_reply":"2023-06-28T20:50:48.466670Z","shell.execute_reply.started":"2023-06-28T20:50:37.494981Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","from kaggle_secrets import UserSecretsClient\n","\n","\n","user_secrets = UserSecretsClient()\n","wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n","wandb.login(key=wandb_api_key)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:50:48.470160Z","iopub.status.busy":"2023-06-28T20:50:48.469236Z","iopub.status.idle":"2023-06-28T20:51:19.412474Z","shell.execute_reply":"2023-06-28T20:51:19.411490Z","shell.execute_reply.started":"2023-06-28T20:50:48.470130Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkokoroou\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.15.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230628_205048-nge8w6n1</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/kokoroou/semantic-segmentation/runs/nge8w6n1' target=\"_blank\">mae_20230628_205048</a></strong> to <a href='https://wandb.ai/kokoroou/semantic-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/kokoroou/semantic-segmentation' target=\"_blank\">https://wandb.ai/kokoroou/semantic-segmentation</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/kokoroou/semantic-segmentation/runs/nge8w6n1' target=\"_blank\">https://wandb.ai/kokoroou/semantic-segmentation/runs/nge8w6n1</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/kokoroou/semantic-segmentation/runs/nge8w6n1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f5ee6466ec0>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# I use wandb to log the hyper-parameters, training results, and model weights.\n","import random\n","from datetime import datetime\n","from pathlib import Path\n","\n","import wandb\n","\n","\n","current_dir = Path(os.getcwd())\n","\n","# Start a new wandb run to track this script\n","wandb.init(\n","    job_type=\"train\",\n","    dir=current_dir,\n","    config=args,\n","    project=\"semantic-segmentation\",\n","    name=\"mae_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n","    notes=\"Masked Autoencoder\",\n","    mode=\"online\"\n",")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:51:19.420091Z","iopub.status.busy":"2023-06-28T20:51:19.417407Z","iopub.status.idle":"2023-06-28T20:51:19.441407Z","shell.execute_reply":"2023-06-28T20:51:19.440384Z","shell.execute_reply.started":"2023-06-28T20:51:19.420055Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","import time\n","\n","import torch\n","\n","\n","def save_model(args, epoch, model, optimizer, criterion):\n","    output_dir = Path(args.output_dir)\n","    checkpoint_path = output_dir / f'checkpoint-{epoch}.pth'\n","\n","    to_save = {\n","        'model': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","        'criterion': criterion.state_dict(),\n","        'epoch': epoch,\n","        'args': args,\n","    }\n","    \n","    torch.save(to_save, checkpoint_path)\n","\n","\n","def save_best_model(args, epoch, model, optimizer, criterion):\n","    output_dir = Path(args.output_dir)\n","    checkpoint_path = output_dir / f'checkpoint-{epoch}.pth'\n","    best_checkpoint_path = output_dir / 'checkpoint-best.pth'\n","\n","    if not checkpoint_path.exists():\n","        to_save = {\n","            'model': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'criterion': criterion.state_dict(),\n","            'epoch': epoch,\n","            'args': args,\n","        }\n","\n","        torch.save(to_save, checkpoint_path)\n","    else:\n","        os.rename(checkpoint_path, best_checkpoint_path)\n","\n","\n","def delete_checkpoint(args, epoch):\n","    # Delete previous checkpoints, except best and last one\n","    output_dir = Path(args.output_dir)\n","\n","    checkpoint_filename = f\"checkpoint-{epoch}.pth\"\n","    checkpoint_path = output_dir / checkpoint_filename\n","    checkpoint_path.unlink(missing_ok=True)\n","    \n","    \n","def save_epoch_result(args, epoch, loss,\n","                      model, optimizer, criterion):\n","    global best_loss\n","    \n","    start = time.time()\n","    print(f\"Save trained model for epoch {epoch}...\")\n","    save_model(args=args, model=model, optimizer=optimizer,\n","               criterion=criterion, epoch=epoch)\n","    print(f\"Save trained model for epoch {epoch}. Finish in {time.time() - start}\")\n","\n","    if loss.item() < best_loss:\n","        best_loss = loss.item()\n","\n","        with open(Path(current_dir) / \"info.txt\", \"w\") as f:\n","            f.write(f\"Best epoch: {epoch}\\nLoss: {loss.item()}\")\n","          \n","        delete_checkpoint(args=args, epoch=\"best\")\n","\n","        start = time.time()\n","        print(\"Save best trained model...\")\n","        save_best_model(\n","            args=args, model=model, optimizer=optimizer,\n","            criterion=criterion, epoch=epoch\n","        )\n","        print(f\"Save best trained model. Finish in {time.time() - start}\")\n","\n","    delete_checkpoint(args=args, epoch=epoch - 1)\n","    "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-28T20:51:36.045928Z","iopub.status.busy":"2023-06-28T20:51:36.045353Z","iopub.status.idle":"2023-06-28T21:19:53.440732Z","shell.execute_reply":"2023-06-28T21:19:52.923166Z","shell.execute_reply.started":"2023-06-28T20:51:36.045888Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialize and add model to GPU...\n","Initialize and add model to GPU. Finish in 14.497633457183838\n","Create dataset...\n","Create dataset. Finish in 12.126553297042847\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 0: 100%|██████████| 101/101 [04:13<00:00,  2.51s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: loss = 0.6975340843200684\n","Save trained model for epoch 0...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1:   0%|          | 0/101 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Save trained model for epoch 0. Finish in 12.108598947525024\n","Save best trained model...\n","Save best trained model. Finish in 0.0011594295501708984\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 101/101 [04:07<00:00,  2.45s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: loss = 0.7071645259857178\n","Save trained model for epoch 1...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2:   0%|          | 0/101 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Save trained model for epoch 1. Finish in 13.894358158111572\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 101/101 [04:12<00:00,  2.50s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: loss = 0.7043012976646423\n","Save trained model for epoch 2...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3:   0%|          | 0/101 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Save trained model for epoch 2. Finish in 13.358202457427979\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 101/101 [03:57<00:00,  2.35s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: loss = 0.6022506356239319\n","Save trained model for epoch 3...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4:   0%|          | 0/101 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Save trained model for epoch 3. Finish in 13.787100076675415\n","Save best trained model...\n","Save best trained model. Finish in 0.0030465126037597656\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 101/101 [03:43<00:00,  2.21s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: loss = 0.6079214811325073\n","Save trained model for epoch 4...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5:   0%|          | 0/101 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Save trained model for epoch 4. Finish in 14.178593158721924\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5: 100%|██████████| 101/101 [03:43<00:00,  2.21s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: loss = 0.5678946375846863\n","Save trained model for epoch 5...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6:   0%|          | 0/101 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Save trained model for epoch 5. Finish in 14.413940906524658\n","Save best trained model...\n","Save best trained model. Finish in 0.0029480457305908203\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6: 100%|██████████| 101/101 [03:38<00:00,  2.16s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6: loss = 0.37699443101882935\n","Save trained model for epoch 6..."]},{"name":"stderr","output_type":"stream","text":["Epoch 7:   0%|          | 0/101 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7:   0%|          | 0/101 [00:08<?, ?it/s]\n","\n","KeyboardInterrupt\n","\n"]}],"source":["import argparse\n","import os\n","from pathlib import Path, PurePosixPath\n","\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from tqdm import tqdm\n","import time\n","from threading import Thread\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","start = time.time()\n","print(\"Initialize and add model to GPU...\")\n","model = MaskedAutoencoderViT()\n","model= nn.DataParallel(model)  # Use all GPU\n","model.to(device)\n","print(f\"Initialize and add model to GPU. Finish in {time.time() - start}\")\n","\n","# Define the path to the directory containing the train images\n","source_dir = Path(current_dir).parent / \"input\" / \"polypgen2021\" / \"PolypGen2021_MultiCenterData_v3\"\n","\n","# Define the path to the file containing the train image names\n","train_filepath = source_dir / \"train_autoencoder.txt\"\n","test_filepath = source_dir / \"test_autoencoder.txt\"\n","\n","# Open the file with names of training, testing image file, then make DataLoader\n","with open(train_filepath, \"r\") as f:\n","    train_filenames = f.read().splitlines()\n","with open(test_filepath, \"r\") as f:\n","    test_filenames = f.read().splitlines()\n","\n","# Create a custom dataset class to load the images\n","class CustomDataset(ImageFolder):\n","    def __init__(self, root, names, transform=None):\n","        super().__init__(root, transform=transform)\n","        self.samples = [\n","            (Path(root, PurePosixPath(name)), 0) for name in names\n","        ]\n","\n","# Define the transformations to apply to the images\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","start = time.time()\n","print(\"Create dataset...\")\n","# Create an instance of the custom dataset\n","train_dataset = CustomDataset(str(source_dir), train_filenames, transform=transform)\n","test_dataset = CustomDataset(str(source_dir), test_filenames, transform=transform)\n","\n","# Create a data loader to load the images in batches\n","batch_size = args.batch_size\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","print(f\"Create dataset. Finish in {time.time() - start}\")\n","\n","# Define the loss function and optimizer\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=wandb.config[\"learning_rate\"])\n","\n","best_loss = 1.0\n","\n","os.makedirs(args.output_dir, exist_ok=True)\n","\n","thread = None\n","\n","for epoch in range(args.epochs):\n","    # Iterate over the data loader batches\n","    for inputs, _ in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n","        inputs = inputs.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, inputs)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","    \n","    # log metrics to wandb\n","    wandb.log({\"loss\": loss.item()})\n","\n","    print(f\"Epoch {epoch}: loss = {loss.item()}\")\n","    \n","    if thread != None:\n","        thread.join()\n","    \n","    # create a thread to save checkpoint in background\n","    thread = Thread(target=save_epoch_result, \n","                    args=(args, epoch, loss, model, optimizer, criterion))\n","    thread.start()\n","\n","# [Optional] Finish the wandb run, necessary in notebooks\n","wandb.finish()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-06-28T21:19:53.498848Z","iopub.status.idle":"2023-06-28T21:19:53.499690Z","shell.execute_reply":"2023-06-28T21:19:53.499428Z","shell.execute_reply.started":"2023-06-28T21:19:53.499402Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Save trained model for epoch 6. Finish in 13.078415632247925\n","Save best trained model...\n","Save best trained model. Finish in 0.0013885498046875\n"]}],"source":["# Giảm GPU Memory bị chiếm dụng khi dừng training giữa chừng\n","import gc\n","\n","gc.collect()\n","torch.cuda.empty_cache()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
